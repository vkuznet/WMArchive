#!/bin/sh
# Author: Valentin Kuznetsov <vkuznet AT gmail [DOT] com>
# A wrapper script to submit spark job with myspark.py script
# from WMArchive project.

# test arguments
if [ "$#" -eq 0 ]; then
    echo "Usage: myspark <options>"
    echo "       myspark --help"
    exit 1
fi

# find out where WMArchive is installed on a system
wmaroot=`python -c "import WMArchive; print '/'.join(WMArchive.__file__.split('/')[:-1])"`
wmaspark=$wmaroot/Tools/myspark.py

# enable simple secret to run in non-yarn mode
conf="--conf spark.authenticate.secret=wmarchive --conf spark.yarn.security.tokens.hive.enabled=false --conf spark.driver.port=5001 --conf spark.blockManager.port=5101 --conf spark.ui.port=5201"

# allow to disable spark output, client should setup his/her own log4j.properties
# via WMA_LOG4J environment variable
# look if we requested to show full log output
if [[ "$@" =~ "--no-log4j" ]]; then
    conf=" --conf spark.ui.showConsoleProgress=false "
    if [ -n "$WMA_LOG4J" ] && [ -f $WMA_LOG4J ]; then
        conf="$conf --conf spark.driver.extraJavaOptions=\"-Dlog4j.configuration=file:$WMA_LOG4J\""
    fi
fi

# set avro jars
avrojar=/usr/lib/avro/avro-mapred.jar
spark2=`spark-submit --version 2>&1 | grep "version 2"`
if [ -n "$spark2" ]; then
    echo "Using spark 2.X"
    jars=""
    conf="$conf --packages com.databricks:spark-avro_2.11:4.0.0"
else
    echo "Using spark 1.X"
    jars="$avrojar"
    if [ -f /usr/hdp/spark-2/examples/jars/spark-examples_2.11-2.3.2.jar ]; then
        sparkexjar=/usr/hdp/spark-2/examples/jars/spark-examples_2.11-2.3.2.jar
        jars="$jars,$sparkexjar"
    else
        sparkexjar=`ls /usr/lib/spark/examples/lib/spark-examples* 2> /dev/null | tail -1`
        if [ -n "$sparkexjar" ]; then
            jars="$jars,$sparkexjar"
        else
            sparkexjar=/usr/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.7.0-hadoop2.6.0-cdh5.7.0.jar
            jars="$jars,$sparkexjar"
        fi
    fi
fi

if [ -n "$jars" ]; then
    jars="--jars $jars"
fi

echo "PYTHONPATH: $PYTHONPATH"
echo "wmaspark: $wmaspark $args"

if [ "$1" == "-h" ] || [ "$1" == "--help" ] || [ "$1" == "-help" ]; then
    # run help
    python $wmaspark --help
elif [[  $1 =~ -?-yarn(-cluster)?$ ]]; then
    # to tune up these numbers:
    #  - executor-memory not more than 5G
    #  - num-executor can be increased (suggested not more than 10)
    #  - cores = 2/4/8
    # Temp solution to have a wrapper for python27 on spark cluster
    # once CERN IT will resolve python version we can remove PYSPARK_PYTHON
    PYSPARK_PYTHON='/afs/cern.ch/user/v/valya/public/python27' \
        spark-submit $jars \
            --master yarn-client \
            --driver-class-path '/usr/lib/hive/lib/*' \
            --driver-java-options '-Dspark.executor.extraClassPath=/usr/lib/hive/lib/*' \
            $conf $wmaspark ${1+"$@"}
else
    # submit spark job with our file, please note
    # that user may increase memory options if necessary
    # the executor and driver memory options can be given in human readable form
    # while spark yarn option should use memoryOverhead as KB value.

    # Modify with local[*] to use all the available cores in the node
    #   optionally increase driver memory with --driver-memory 2G (default 1G)
    PYSPARK_PYTHON='/afs/cern.ch/user/v/valya/public/python27' \
    spark-submit $jars \
        --executor-memory $((`nproc`/4))G \
        --driver-memory 4G \
        --master local[$((`nproc`/4))] \
        --driver-class-path '/usr/lib/hive/lib/*' \
        --driver-java-options '-Dspark.executor.extraClassPath=/usr/lib/hive/lib/*' \
        $conf $wmaspark ${1+"$@"}
# Original example
# submit spark job with our file, please note
# that user may increase memory options if necessary
# the executor and driver memory options can be given in human readable form
# while spark yarn option should use memoryOverhead as KB value
#    spark-submit --driver-class-path $avrojar:$sparkexjar \
#        --executor-memory 4G \
#        --driver-memory 4G \
#        --conf spark.yarn.executor.memoryOverhead=4096 \
#        $wmaroot/Tools/myspark.py ${1+"$@"}
fi
